import os
import streamlit as st
import requests

# ======================================================
# 1. KONFIGURASI HALAMAN
# ======================================================
st.set_page_config(
    page_title="SMARDOS - Ruang Konsultasi",
    page_icon="ğŸ“",
    layout="wide",
    initial_sidebar_state="expanded"
)

# ======================================================
# 2. ADVANCED CUSTOM CSS (FIX KONTRAS TEKS)
# ======================================================
st.markdown(
    """
    <style>
    @import url('https://fonts.googleapis.com/css2?family=Nunito:wght@400;600;800&display=swap');

    /* Base */
    .stApp { background-color: #f8fafc; }
    html, body, [class*="st-"] { font-family: 'Nunito', sans-serif; }

    /* Sidebar */
    [data-testid="stSidebar"] {
        background-color: #ffffff !important;
        border-right: 1px solid #e2e8f0;
    }

    /* Buttons */
    .stButton>button {
        width: 100%;
        border-radius: 10px;
        font-weight: 600;
        text-align: left;
        padding: 10px 15px;
    }

    /* Header */
    .main-header {
        background: white;
        padding: 1rem 2rem;
        border-bottom: 2px solid #2563eb;
        display: flex;
        justify-content: space-between;
        align-items: center;
        margin-bottom: 20px;
        border-radius: 14px;
    }

    /* ===== FIX: Chat bubble contrast on Dark/Light mode =====
       Streamlit chat message container: [data-testid="stChatMessage"]
       Role markers are classes: .stChatMessage (newer) / data attributes (varies).
       We'll style by 'assistant' and 'user' via attribute selector on avatar container parent.
    */

    /* Make chat message look consistent */
    [data-testid="stChatMessage"] {
        border-radius: 14px;
        padding: 1.0rem 1.1rem;
        margin-bottom: 0.8rem;
        border: 1px solid rgba(148,163,184,0.35);
    }

    /* Force readable text inside chat */
    [data-testid="stChatMessage"] * {
        color: #0f172a !important; /* slate-900 */
    }

    /* Assistant bubble background */
    [data-testid="stChatMessage"][data-testid*="assistant"],
    [data-testid="stChatMessage"]:has([aria-label="assistant avatar"]) {
        background: #ffffff !important;
    }

    /* User bubble background */
    [data-testid="stChatMessage"][data-testid*="user"],
    [data-testid="stChatMessage"]:has([aria-label="user avatar"]) {
        background: #eff6ff !important; /* blue-50 */
        border-color: rgba(37, 99, 235, 0.25) !important;
    }

    /* If browser/theme dark mode makes page dark, keep bubbles readable */
    @media (prefers-color-scheme: dark) {
        .stApp { background-color: #0b1220; } /* dark */
        [data-testid="stSidebar"] { background-color: #0f172a !important; border-right: 1px solid rgba(148,163,184,0.15); }
        [data-testid="stChatMessage"] { border-color: rgba(148,163,184,0.18); }
        [data-testid="stChatMessage"] * { color: #0f172a !important; } /* keep text dark because bubbles are light */
    }

    /* Optional: make markdown links visible */
    [data-testid="stChatMessage"] a {
        color: #2563eb !important;
        text-decoration: underline;
    }
    </style>
    """,
    unsafe_allow_html=True
)

# ======================================================
# 3. KONFIG OLLAMA
# ======================================================
OLLAMA_BASE_URL = os.getenv("OLLAMA_BASE_URL", "http://host.docker.internal:11434").rstrip("/")

# ======================================================
# 4. LOGIC FUNCTIONS
# ======================================================
@st.cache_data(ttl=300)
def get_available_models(base_url: str):
    try:
        r = requests.get(f"{base_url}/api/tags", timeout=5)
        r.raise_for_status()
        data = r.json()
        return [m["name"] for m in data.get("models", [])]
    except Exception:
        return []


def generate_smardos_response(user_input: str, model_name: str, base_url: str) -> str:
    try:
        from langchain_ollama import OllamaLLM
    except ModuleNotFoundError:
        return "Ollama (langchain_ollama) tidak tersedia di environment ini."

    llm = OllamaLLM(
        model=model_name,
        temperature=0.3,
        base_url=base_url,
        # Biar lebih cepat dan tidak kebablasan panjang
        num_predict=350
    )

    system_instructions = (
        "Kamu adalah SMARDOS (Smart Asisten Dosen), asisten akademik khusus perguruan tinggi.\n"
        "TUGAS UTAMA:\n"
        "1. Hanya jawab pertanyaan yang berkaitan dengan materi perkuliahan, teori akademik, atau metode penelitian.\n"
        "2. Jika pertanyaan TIDAK berkaitan dengan perkuliahan/pendidikan, tolak dengan sopan.\n"
        "3. Setiap jawaban WAJIB menyertakan referensi ilmiah di bagian akhir.\n"
        "4. Format referensi harus mencantumkan Link URL (Google Scholar/DOAJ/dll).\n"
        "5. Gunakan Bahasa Indonesia yang formal dan edukatif.\n"
        "6. Jawaban maksimal 6 paragraf, ringkas dan jelas."
    )

    full_prompt = f"### Instruction:\n{system_instructions}\n\n### User Question:\n{user_input}\n\n### Response:"
    return llm.invoke(full_prompt)


def check_ollama_connection(base_url: str):
    try:
        r = requests.get(f"{base_url}/api/tags", timeout=5)
        st.write("âœ… Status:", r.status_code)
        st.write("âœ… Endpoint:", f"{base_url}/api/tags")
        st.json(r.json())
    except Exception as e:
        st.error(f"âŒ Gagal konek ke Ollama: {e}")

# ======================================================
# 5. SIDEBAR
# ======================================================
with st.sidebar:
    st.markdown(
        "<h1 style='color:#1e3a8a;font-size:24px;font-weight:800;'>ğŸ“ SMARDOS</h1>",
        unsafe_allow_html=True
    )
    st.caption("v2.5 - Your Smart Academic Partner")

    st.markdown("---")

    st.subheader("ğŸ“Œ Navigasi")
    if st.button("ğŸ  Kembali ke Beranda"):
        st.switch_page("home.py")

    st.markdown("---")

    st.subheader("âš™ï¸ Panel Kontrol AI")
    st.caption(f"ğŸ”— Ollama Base URL: `{OLLAMA_BASE_URL}`")

    available_models = get_available_models(OLLAMA_BASE_URL)

    if available_models:
        selected_model = st.selectbox("Pilih Model Ollama", available_models)
        st.success(f"Model aktif: **{selected_model}**")
    else:
        selected_model = None
        st.warning("Ollama terhubung, tapi belum ada model. Jalankan `ollama pull ...` di host dulu.")

    if st.button("ğŸ” Cek Koneksi Ollama"):
        check_ollama_connection(OLLAMA_BASE_URL)

    st.markdown("---")

    if st.button("ğŸ—‘ï¸ Bersihkan Riwayat Chat"):
        st.session_state.messages = []
        st.rerun()

# ======================================================
# 6. MAIN CONTENT
# ======================================================
st.markdown(
    """
    <div class="main-header">
        <div style="display:flex;align-items:center;gap:10px;">
            <span style="font-size:24px;">ğŸ’¬</span>
            <h3 style="margin:0;color:#1e3a8a;">Konsultasi Akademik</h3>
        </div>
        <div style="background:#eff6ff;padding:5px 15px;border-radius:20px;
                    color:#2563eb;font-weight:bold;font-size:12px;">
            STATUS: ONLINE
        </div>
    </div>
    """,
    unsafe_allow_html=True
)

if "messages" not in st.session_state:
    st.session_state.messages = [
        {
            "role": "assistant",
            "content": (
                "Halo, saya **SMARDOS**. "
                "Silakan pilih model AI di sidebar dan mulai berdiskusi."
            )
        }
    ]

for message in st.session_state.messages:
    with st.chat_message(
        message["role"],
        avatar="ğŸ“" if message["role"] == "assistant" else "ğŸ‘¤"
    ):
        st.markdown(message["content"])

if selected_model:
    if prompt := st.chat_input("Tanyakan sesuatu pada SMARDOS"):
        st.session_state.messages.append({"role": "user", "content": prompt})

        with st.chat_message("user", avatar="ğŸ‘¤"):
            st.markdown(prompt)

        with st.chat_message("assistant", avatar="ğŸ“"):
            with st.spinner("Sedang memproses"):
                response = generate_smardos_response(prompt, selected_model, OLLAMA_BASE_URL)
                st.markdown(response)
                st.session_state.messages.append({"role": "assistant", "content": response})
else:
    st.info("Pilih model di sidebar untuk memulai.")

st.markdown(
    "<div style='text-align:center;color:#94a3b8;font-size:12px;margin-top:50px;'>"
    "SMARDOS Local Intelligence"
    "</div>",
    unsafe_allow_html=True
)
